{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab10: NN, ReLu, Xavier, Dropout, and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.457027943\n",
      "Epoch: 0002 cost = 0.169663619\n",
      "Epoch: 0003 cost = 0.130157096\n",
      "Epoch: 0004 cost = 0.108546730\n",
      "Epoch: 0005 cost = 0.092537919\n",
      "Epoch: 0006 cost = 0.083682013\n",
      "Epoch: 0007 cost = 0.074270519\n",
      "Epoch: 0008 cost = 0.068983357\n",
      "Epoch: 0009 cost = 0.062872721\n",
      "Epoch: 0010 cost = 0.060662401\n",
      "Epoch: 0011 cost = 0.056342752\n",
      "Epoch: 0012 cost = 0.051641211\n",
      "Epoch: 0013 cost = 0.052601567\n",
      "Epoch: 0014 cost = 0.050167573\n",
      "Epoch: 0015 cost = 0.045412711\n",
      "Learning Finished!\n",
      "Accuracy: 0.9816\n",
      "Label:  [3]\n",
      "Prediction:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QeC+hPiQVe7KFqy5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f6516d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
